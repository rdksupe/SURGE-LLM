{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jivnesh/anaconda3/envs/RishiSurge/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Test dataframe created with 4000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 05:47:47.838499: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-15 05:47:48.033917: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752526068.127124 2870596 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752526068.143688 2870596 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752526068.395860 2870596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752526068.395896 2870596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752526068.395898 2870596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752526068.395900 2870596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-15 05:47:48.437353: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and set to evaluation mode.\n",
      "Style centroids loaded.\n",
      "\n",
      "Starting evaluation on 4000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 125/125 [01:17<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "      Benchmark Evaluation Results\n",
      "========================================\n",
      "\n",
      "--- Classification Report ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Human (Class 0)       0.94      0.99      0.97      2000\n",
      "   AI (Class 1)       0.99      0.94      0.96      2000\n",
      "\n",
      "       accuracy                           0.96      4000\n",
      "      macro avg       0.97      0.96      0.96      4000\n",
      "   weighted avg       0.97      0.96      0.96      4000\n",
      "\n",
      "Overall Accuracy: 0.9645\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "                | Predicted Human | Predicted AI   \n",
      "--------------------------------------------------\n",
      "Actual Human    | 1985            | 15             \n",
      "Actual AI       | 127             | 1873           \n",
      "--------------------------------------------------\n",
      "\n",
      "False Positive Rate (FPR): 0.0075 (0.75%)\n",
      "(Percentage of human texts incorrectly flagged as AI)\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda:03' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DEFINE THE MODEL ARCHITECTURE\n",
    "# This must match the architecture of the saved model.\n",
    "# ==============================================================================\n",
    "class StyleContrastiveEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 base_model=\"microsoft/deberta-v3-base\",\n",
    "                 embedding_dim=256,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(base_model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "        \n",
    "        backbone_dim = self.backbone.config.hidden_size\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, backbone_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(backbone_dim // 2, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Use clamp to avoid division by zero for empty attention masks\n",
    "        attention_weights = attention_mask.unsqueeze(-1).float()\n",
    "        pooled = (hidden_states * attention_weights).sum(1) / attention_weights.sum(1).clamp(min=1e-9)\n",
    "        style_embedding = self.projection_head(pooled)\n",
    "        return F.normalize(style_embedding, p=2, dim=1)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DEFINE THE DETECTOR CLASS FOR INFERENCE\n",
    "# ==============================================================================\n",
    "class StyleDetector:\n",
    "    \"\"\"An efficient class for loading the model and running batch predictions.\"\"\"\n",
    "    def __init__(self, model_path, centroids_path, device):\n",
    "        self.device = device\n",
    "        \n",
    "        # Load model architecture\n",
    "        self.model = StyleContrastiveEncoder().to(self.device)\n",
    "        \n",
    "        # Load the saved model weights\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Handle models saved with nn.DataParallel (which add a 'module.' prefix)\n",
    "        state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "        if list(state_dict.keys())[0].startswith('module.'):\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:]  # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            self.model.load_state_dict(new_state_dict)\n",
    "        else:\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            \n",
    "        self.model.eval()\n",
    "        print(\"Model loaded and set to evaluation mode.\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        \n",
    "        # Load style centroids\n",
    "        centroids = torch.load(centroids_path, map_location=device)\n",
    "        self.human_centroid = centroids['human_centroid'].to(self.device).unsqueeze(0)\n",
    "        self.gpt4_centroid = centroids['gpt4_centroid'].to(self.device).unsqueeze(0)\n",
    "        print(\"Style centroids loaded.\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def batch_predict(self, texts: list, batch_size: int = 32):\n",
    "        \"\"\"Predicts labels for a list of texts in batches.\"\"\"\n",
    "        all_predictions = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Benchmarking\"):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            \n",
    "            tokens = self.tokenizer(\n",
    "                batch_texts,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            embeddings = self.model(tokens['input_ids'], tokens['attention_mask'])\n",
    "            \n",
    "            # Compare similarity to centroids\n",
    "            human_sims = F.cosine_similarity(embeddings, self.human_centroid)\n",
    "            gpt4_sims = F.cosine_similarity(embeddings, self.gpt4_centroid)\n",
    "            \n",
    "            # Prediction: 1 if closer to AI centroid, 0 if closer to human\n",
    "            predictions = (gpt4_sims > human_sims).long()\n",
    "            all_predictions.extend(predictions.cpu().tolist())\n",
    "            \n",
    "        return all_predictions\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DEFINE THE EVALUATION FUNCTION\n",
    "# ==============================================================================\n",
    "def evaluate_on_dataframe(detector: StyleDetector, df: pd.DataFrame):\n",
    "    \"\"\"Runs a full evaluation on a DataFrame and prints a detailed report.\"\"\"\n",
    "    print(f\"\\nStarting evaluation on {len(df)} samples...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    texts_to_evaluate = df['text'].tolist()\n",
    "    # Convert 'human'/'ai' labels to 0/1 for scikit-learn\n",
    "    ground_truth_labels = df['models'].apply(lambda x: 0 if x.lower() == 'human' else 1).tolist()\n",
    "    \n",
    "    # Get predictions from the detector\n",
    "    predictions = detector.batch_predict(texts_to_evaluate)\n",
    "    \n",
    "    # --- Calculate and Print Metrics ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"      Benchmark Evaluation Results\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Classification Report (Precision, Recall, F1-Score)\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(ground_truth_labels, predictions, target_names=['Human (Class 0)', 'AI (Class 1)']))\n",
    "    \n",
    "    # Overall Accuracy\n",
    "    accuracy = accuracy_score(ground_truth_labels, predictions)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(ground_truth_labels, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"{'':<15} | {'Predicted Human':<15} | {'Predicted AI':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Actual Human':<15} | {tn:<15} | {fp:<15}\")\n",
    "    print(f\"{'Actual AI':<15} | {fn:<15} | {tp:<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # False Positive Rate (FPR)\n",
    "    if (fp + tn) > 0:\n",
    "        fpr = fp / (fp + tn)\n",
    "        print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f} ({fpr:.2%})\")\n",
    "        print(\"(Percentage of human texts incorrectly flagged as AI)\")\n",
    "    else:\n",
    "        print(\"\\nFalse Positive Rate (FPR): N/A (No human samples in test set)\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- Load and Prepare Your Dataset ---\n",
    "    df = pd.read_csv(\"/home/jivnesh/Harshit_Surge/dataset/sampled_train.csv\")\n",
    "    data = pd.DataFrame()\n",
    "    data[\"models\"] = df[\"model\"].apply(lambda x: \"ai\" if x != \"human\" else \"human\")\n",
    "    data['text'] = df['title'] + \" \" + df['generation']\n",
    "    data.dropna(subset=['text'], inplace=True) # Ensure no null texts\n",
    "\n",
    "    df_human = data[data.models == \"human\"]\n",
    "    df_ai = data[data.models == \"ai\"]\n",
    "    \n",
    "    # Create balanced train/test splits\n",
    "    train_human = df_human.sample(n=10000, random_state=42)\n",
    "    train_ai = df_ai.sample(n=10000, random_state=42)\n",
    "    test_human = df_human.drop(train_human.index).sample(n=2000, random_state=42)\n",
    "    test_ai = df_ai.drop(train_ai.index).sample(n=2000, random_state=42)\n",
    "\n",
    "    train_df = pd.concat([train_human, train_ai], axis=0)\n",
    "    test_df = pd.concat([test_human, test_ai], axis=0)\n",
    "\n",
    "    # Shuffle the final test dataframe\n",
    "    test_df = shuffle(test_df, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Test dataframe created with {len(test_df)} samples.\")\n",
    "    \n",
    "    # --- Set Paths to Your Saved Model and Centroids ---\n",
    "    MODEL_PATH = 'best_style_model.pt'\n",
    "    CENTROIDS_PATH = 'centroids.pt'\n",
    "\n",
    "    # Check if files exist before proceeding\n",
    "    if not os.path.exists(MODEL_PATH) or not os.path.exists(CENTROIDS_PATH):\n",
    "        print(f\"Error: Make sure '{MODEL_PATH}' and '{CENTROIDS_PATH}' are in the correct directory.\")\n",
    "    else:\n",
    "        # --- Initialize the Detector and Run Evaluation ---\n",
    "        detector = StyleDetector(model_path=MODEL_PATH, centroids_path=CENTROIDS_PATH, device=device)\n",
    "        evaluate_on_dataframe(detector, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jivnesh/anaconda3/envs/RishiSurge/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing the dataset...\n",
      "--------------------------------------------------\n",
      "Test DataFrame created for benchmarking.\n",
      "Total samples: 4000\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Starting Binoculars Evaluation ---\n",
      "Initializing Binoculars with Gemma-2-9B models...\n",
      "Loading observer model: tiiuae/falcon-7b onto cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "2025-07-15 06:54:03.398055: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-15 06:54:03.427546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752530043.452627 2908091 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752530043.460137 2908091 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752530043.479395 2908091 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752530043.479417 2908091 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752530043.479420 2908091 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752530043.479422 2908091 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-15 06:54:03.490276: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading performer model: tiiuae/falcon-7b-instruct onto cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binoculars initialized successfully.\n",
      "\n",
      "Running predictions on 4000 samples in batches of 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 500/500 [2:53:53<00:00, 20.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "      Binoculars Classification Statistics\n",
      "==================================================\n",
      "\n",
      "--- Classification Report ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Human-generated       0.88      0.80      0.84      2000\n",
      "   AI-generated       0.81      0.89      0.85      2000\n",
      "\n",
      "       accuracy                           0.84      4000\n",
      "      macro avg       0.85      0.84      0.84      4000\n",
      "   weighted avg       0.85      0.84      0.84      4000\n",
      "\n",
      "\n",
      "Overall Accuracy: 0.8440\n",
      "ROC AUC Score: 0.9295\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "                | Predicted Human | Predicted AI   \n",
      "--------------------------------------------------\n",
      "Actual Human    | 1784            | 216            \n",
      "Actual AI       | 408             | 1592           \n",
      "--------------------------------------------------\n",
      "\n",
      "False Positive Rate (FPR): 0.1080 (Human text incorrectly flagged as AI)\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Union, List\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# --- Pre-computation Setup & Warnings ---\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# --- Metric Functions (Unchanged) ---\n",
    "ce_loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "softmax_fn = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "def perplexity(encoding: transformers.BatchEncoding, logits: torch.Tensor, median: bool = False, temperature: float = 1.0):\n",
    "    shifted_logits = logits[..., :-1, :].contiguous() / temperature\n",
    "    shifted_labels = encoding.input_ids[..., 1:].contiguous()\n",
    "    shifted_attention_mask = encoding.attention_mask[..., 1:].contiguous()\n",
    "    if median:\n",
    "        ce_nan = (ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels).masked_fill(~shifted_attention_mask.bool(), float(\"nan\")))\n",
    "        ppl = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n",
    "    else:\n",
    "        ppl = (ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels) * shifted_attention_mask).sum(1) / shifted_attention_mask.sum(1)\n",
    "        ppl = ppl.to(\"cpu\").float().numpy()\n",
    "    return ppl\n",
    "\n",
    "def entropy(p_logits: torch.Tensor, q_logits: torch.Tensor, encoding: transformers.BatchEncoding, pad_token_id: int, median: bool = False, sample_p: bool = False, temperature: float = 1.0):\n",
    "    vocab_size = p_logits.shape[-1]\n",
    "    total_tokens_available = q_logits.shape[-2]\n",
    "    p_scores, q_scores = p_logits / temperature, q_logits / temperature\n",
    "    p_proba = softmax_fn(p_scores).view(-1, vocab_size)\n",
    "    if sample_p:\n",
    "        p_proba = torch.multinomial(p_proba.view(-1, vocab_size), replacement=True, num_samples=1).view(-1)\n",
    "    q_scores = q_scores.view(-1, vocab_size)\n",
    "    ce = ce_loss_fn(input=q_scores, target=p_proba).view(-1, total_tokens_available)\n",
    "    padding_mask = (encoding.input_ids != pad_token_id).type(torch.uint8)\n",
    "    if median:\n",
    "        ce_nan = ce.masked_fill(~padding_mask.bool(), float(\"nan\"))\n",
    "        agg_ce = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n",
    "    else:\n",
    "        agg_ce = (((ce * padding_mask).sum(1) / padding_mask.sum(1)).to(\"cpu\").float().numpy())\n",
    "    return agg_ce\n",
    "\n",
    "# --- Binoculars Classifier (MODIFIED to use Gemma) ---\n",
    "huggingface_config = {\"TOKEN\": os.environ.get(\"HF_TOKEN\", None)}\n",
    "# NOTE: The threshold is based on the original Falcon models.\n",
    "# Performance may vary with Gemma, but we use the \"accuracy\" mode threshold.\n",
    "BINOCULARS_ACCURACY_THRESHOLD = 0.9015310749276843\n",
    "DEVICE_1 = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE_2 = \"cuda:1\" if torch.cuda.device_count() > 1 else DEVICE_1\n",
    "\n",
    "class Binoculars(object):\n",
    "    def __init__(self, observer_name_or_path: str = \"tiiuae/falcon-7b\", performer_name_or_path: str = \"tiiuae/falcon-7b-instruct\", use_bfloat16: bool = True, max_token_observed: int = 512) -> None:\n",
    "        print(\"Initializing Binoculars with Gemma-2-9B models...\")\n",
    "        self.threshold = BINOCULARS_ACCURACY_THRESHOLD\n",
    "        print(f\"Loading observer model: {observer_name_or_path} onto {DEVICE_1}\")\n",
    "        self.observer_model = AutoModelForCausalLM.from_pretrained(observer_name_or_path, device_map={\"\": DEVICE_1}, trust_remote_code=True, torch_dtype=torch.bfloat16 if use_bfloat16 else torch.float32, token=huggingface_config[\"TOKEN\"])\n",
    "        print(f\"Loading performer model: {performer_name_or_path} onto {DEVICE_2}\")\n",
    "        self.performer_model = AutoModelForCausalLM.from_pretrained(performer_name_or_path, device_map={\"\": DEVICE_2}, trust_remote_code=True, torch_dtype=torch.bfloat16 if use_bfloat16 else torch.float32, token=huggingface_config[\"TOKEN\"])\n",
    "        self.observer_model.eval(); self.performer_model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(observer_name_or_path, token=huggingface_config[\"TOKEN\"])\n",
    "        if not self.tokenizer.pad_token: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.max_token_observed = max_token_observed\n",
    "        print(\"Binoculars initialized successfully.\")\n",
    "    def _tokenize(self, batch: list[str]) -> transformers.BatchEncoding:\n",
    "        return self.tokenizer(batch, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=self.max_token_observed, return_token_type_ids=False)\n",
    "    @torch.inference_mode()\n",
    "    def _get_logits(self, encodings: transformers.BatchEncoding) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        observer_logits = self.observer_model(**encodings.to(DEVICE_1)).logits\n",
    "        performer_logits = self.performer_model(**encodings.to(DEVICE_2)).logits\n",
    "        if torch.cuda.is_available() and DEVICE_1 != \"cpu\": torch.cuda.synchronize()\n",
    "        return observer_logits, performer_logits\n",
    "    def compute_score(self, input_text: Union[str, List[str]]) -> Union[float, List[float]]:\n",
    "        batch = [input_text] if isinstance(input_text, str) else input_text\n",
    "        encodings = self._tokenize(batch)\n",
    "        observer_logits, performer_logits = self._get_logits(encodings)\n",
    "        ppl_val = perplexity(encodings.to(DEVICE_2), performer_logits)\n",
    "        x_ppl_val = entropy(observer_logits.to(DEVICE_1), performer_logits.to(DEVICE_1), encodings.to(DEVICE_1), self.tokenizer.pad_token_id)\n",
    "        binoculars_scores = ppl_val / x_ppl_val\n",
    "        return binoculars_scores.tolist()[0] if isinstance(input_text, str) else binoculars_scores.tolist()\n",
    "\n",
    "# --- Evaluation Function (MODIFIED to add full stats) ---\n",
    "def evaluate_on_dataframe(df: pd.DataFrame, batch_size: int = 8):\n",
    "    print(\"\\n--- Starting Binoculars Evaluation ---\")\n",
    "    try:\n",
    "        binoculars = Binoculars()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- ERROR ---\"); print(f\"Failed to initialize Binoculars classifier: {e}\"); return\n",
    "\n",
    "    text_samples = df[\"text\"].tolist()\n",
    "    true_labels = df[\"models\"].tolist()\n",
    "    all_predictions, all_scores = [], []\n",
    "\n",
    "    print(f\"\\nRunning predictions on {len(text_samples)} samples in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(text_samples), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_texts = text_samples[i:i + batch_size]\n",
    "        batch_scores = binoculars.compute_score(batch_texts)\n",
    "        batch_predictions = np.where(np.array(batch_scores) < binoculars.threshold, \"AI-generated\", \"Human-generated\").tolist()\n",
    "        all_predictions.extend(batch_predictions); all_scores.extend(batch_scores)\n",
    "    \n",
    "    results_df = pd.DataFrame({'true_label': [\"AI-generated\" if l == 'ai' else \"Human-generated\" for l in true_labels], 'predicted_label': all_predictions, 'binoculars_score': all_scores})\n",
    "    \n",
    "    # Prepare labels for sklearn metrics\n",
    "    y_true_str = results_df['true_label']\n",
    "    y_pred_str = results_df['predicted_label']\n",
    "    y_true_bin = (y_true_str == 'AI-generated').astype(int)\n",
    "    y_pred_bin = (y_pred_str == 'AI-generated').astype(int)\n",
    "    y_scores = -results_df['binoculars_score'].values\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"      Binoculars Classification Statistics\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Classification Report (Precision, Recall, F1-Score)\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_true_str, y_pred_str, target_names=['Human-generated', 'AI-generated']))\n",
    "\n",
    "    # 2. Overall Accuracy\n",
    "    accuracy = accuracy_score(y_true_str, y_pred_str)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # 3. ROC AUC Score\n",
    "    try:\n",
    "        auc_score = roc_auc_score(y_true_bin, y_scores)\n",
    "        print(f\"ROC AUC Score: {auc_score:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate ROC AUC Score: {e}\")\n",
    "\n",
    "    # 4. Confusion Matrix and FPR\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true_bin, y_pred_bin)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        \n",
    "        print(\"\\n--- Confusion Matrix ---\")\n",
    "        print(f\"{'':<15} | {'Predicted Human':<15} | {'Predicted AI':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Actual Human':<15} | {tn:<15} | {fp:<15}\")\n",
    "        print(f\"{'Actual AI':<15} | {fn:<15} | {tp:<15}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f} (Human text incorrectly flagged as AI)\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate Confusion Matrix or FPR: {e}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# --- Main Execution ---\n",
    "print(\"Loading and preparing the dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/jivnesh/Harshit_Surge/dataset/sampled_train.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File '/home/jivnesh/Harshit_Surge/dataset/sampled_train.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"models\"] = df[\"model\"].apply(lambda x: \"ai\" if x != \"human\" else \"human\")\n",
    "data['text'] = df['title'] + \" \" + df['generation']\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "df_human = data[data.models == \"human\"]\n",
    "df_ai = data[data.models == \"ai\"]\n",
    "\n",
    "train_human = df_human.sample(n=10000, random_state=42)\n",
    "train_ai = df_ai.sample(n=10000, random_state=42)\n",
    "test_human = df_human.drop(train_human.index).sample(n=2000, random_state=42)\n",
    "test_ai = df_ai.drop(train_ai.index).sample(n=2000, random_state=42)\n",
    "\n",
    "test_df = pd.concat([test_human, test_ai], axis=0)\n",
    "test_df = shuffle(test_df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Test DataFrame created for benchmarking.\")\n",
    "print(f\"Total samples: {len(test_df)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_on_dataframe(test_df, batch_size=8) # Smaller batch size for larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# A dictionary to map short names to Hugging Face model identifiers\n",
    "model_fullnames = {\n",
    "    'gemma3-4b': 'google/gemma-3-4b-it', # Use instruct-tuned version for better performance\n",
    "}\n",
    "\n",
    "def get_model_fullname(model_name):\n",
    "    return model_fullnames.get(model_name, model_name)\n",
    "\n",
    "def load_model(model_name, device, cache_dir, quantization=None):\n",
    "    model_fullname = get_model_fullname(model_name)\n",
    "    print(f'Loading model {model_fullname}...')\n",
    "    model_kwargs = {\"cache_dir\": cache_dir}\n",
    "    print(\"-> Loading model in bfloat16 (half-precision)...\")\n",
    "    model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
    "    model_kwargs[\"device_map\"] = \"auto\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_fullname, **model_kwargs)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(model_name, cache_dir):\n",
    "    model_fullname = get_model_fullname(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_fullname, cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return tokenizer\n",
    "\n",
    "def get_sampling_discrepancy_analytic(logits_ref, logits_score, labels):\n",
    "    if logits_ref.size(-1) != logits_score.size(-1):\n",
    "        vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n",
    "        logits_ref = logits_ref[:, :, :vocab_size]\n",
    "        logits_score = logits_score[:, :, :vocab_size]\n",
    "    labels = labels.unsqueeze(-1) if labels.ndim == logits_score.ndim - 1 else labels\n",
    "    lprobs_score = torch.log_softmax(logits_score, dim=-1)\n",
    "    probs_ref = torch.softmax(logits_ref, dim=-1)\n",
    "    log_likelihood = lprobs_score.gather(dim=-1, index=labels).squeeze(-1)\n",
    "    mean_ref = (probs_ref * lprobs_score).sum(dim=-1)\n",
    "    var_ref = (probs_ref * torch.square(lprobs_score)).sum(dim=-1) - torch.square(mean_ref)\n",
    "    log_likelihood_sum = log_likelihood.sum(dim=-1)\n",
    "    mean_ref_sum = mean_ref.sum(dim=-1)\n",
    "    var_ref_sum = var_ref.sum(dim=-1)\n",
    "    denominator = torch.sqrt(torch.relu(var_ref_sum)) + 1e-6\n",
    "    discrepancy = (log_likelihood_sum - mean_ref_sum) / denominator\n",
    "    return discrepancy.item()\n",
    "\n",
    "def compute_prob_norm(x, mu0, sigma0, mu1, sigma1):\n",
    "    pdf_value0 = norm.pdf(x, loc=mu0, scale=sigma0)\n",
    "    pdf_value1 = norm.pdf(x, loc=mu1, scale=sigma1)\n",
    "    prob = pdf_value1 / (pdf_value0 + pdf_value1 + 1e-6)\n",
    "    return prob\n",
    "\n",
    "class FastDetectGPTDetector:\n",
    "    def __init__(self, scoring_model_name, sampling_model_name, device, cache_dir, quantization):\n",
    "        self.scoring_model_name = scoring_model_name\n",
    "        self.sampling_model_name = sampling_model_name\n",
    "        self.scoring_tokenizer = load_tokenizer(scoring_model_name, cache_dir)\n",
    "        self.scoring_model = load_model(scoring_model_name, device, cache_dir, quantization)\n",
    "        if sampling_model_name == scoring_model_name:\n",
    "            self.sampling_model = self.scoring_model\n",
    "            self.sampling_tokenizer = self.scoring_tokenizer\n",
    "        else:\n",
    "            self.sampling_tokenizer = load_tokenizer(sampling_model_name, cache_dir)\n",
    "            self.sampling_model = load_model(sampling_model_name, device, cache_dir, quantization)\n",
    "        # Using pre-calibrated parameters\n",
    "        self.classifier_params = {'mu0': -0.0707, 'sigma0': 0.9520, 'mu1': 2.9306, 'sigma1': 1.9039}\n",
    "\n",
    "    def compute_prob(self, text):\n",
    "        tokenized_score = self.scoring_tokenizer(text, truncation=True, return_tensors=\"pt\", max_length=1024)\n",
    "        labels = tokenized_score.input_ids[:, 1:].to(self.scoring_model.device)\n",
    "        if labels.shape[1] == 0:\n",
    "            return 0.0\n",
    "        with torch.no_grad():\n",
    "            inputs_score = {k: v.to(self.scoring_model.device) for k, v in tokenized_score.items()}\n",
    "            logits_score = self.scoring_model(**inputs_score).logits[:, :-1]\n",
    "            if self.sampling_model_name == self.scoring_model_name:\n",
    "                logits_ref = logits_score\n",
    "            else:\n",
    "                tokenized_ref = self.sampling_tokenizer(text, truncation=True, return_tensors=\"pt\", max_length=1024)\n",
    "                inputs_ref = {k: v.to(self.sampling_model.device) for k, v in tokenized_ref.items()}\n",
    "                logits_ref = self.sampling_model(**inputs_ref).logits[:, :-1]\n",
    "        crit = get_sampling_discrepancy_analytic(logits_ref, logits_score, labels)\n",
    "        prob = compute_prob_norm(crit, **self.classifier_params)\n",
    "        return prob\n",
    "\n",
    "# --- Script Configuration ---\n",
    "SCORING_MODEL_NAME = \"gemma3-4b\"\n",
    "SAMPLING_MODEL_NAME = \"gemma3-4b\" # Use the same model for simplicity\n",
    "DEVICE = \"cuda:03\" if torch.cuda.is_available() else \"cpu\"\n",
    "CACHE_DIR = \"./model_cache\"\n",
    "OUTPUT_FILE = \"fastdetectgpt_gemma_results.csv\"\n",
    "\n",
    "# --- Main Execution ---\n",
    "print(\"--- Initializing Fast-DetectGPT Detector ---\")\n",
    "detector = FastDetectGPTDetector(\n",
    "    scoring_model_name=SCORING_MODEL_NAME,\n",
    "    sampling_model_name=SAMPLING_MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization=None\n",
    ")\n",
    "\n",
    "print(\"\\n--- Loading and preparing dataset ---\")\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/jivnesh/Harshit_Surge/dataset/sampled_train.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File '/home/jivnesh/Harshit_Surge/dataset/sampled_train.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"models\"] = df[\"model\"].apply(lambda x: \"ai\" if x != \"human\" else \"human\")\n",
    "data['text'] = df['title'] + \" \" + df['generation']\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "df_human = data[data.models == \"human\"]\n",
    "df_ai = data[data.models == \"ai\"]\n",
    "\n",
    "train_human = df_human.sample(n=10000, random_state=42)\n",
    "train_ai = df_ai.sample(n=10000, random_state=42)\n",
    "test_human = df_human.drop(train_human.index).sample(n=2000, random_state=42)\n",
    "test_ai = df_ai.drop(train_ai.index).sample(n=2000, random_state=42)\n",
    "\n",
    "test_df = pd.concat([test_human, test_ai], axis=0)\n",
    "test_df = shuffle(test_df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Test DataFrame created with {len(test_df)} samples.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n--- Running detection on {len(test_df)} samples ---\")\n",
    "all_probs = []\n",
    "true_labels = []\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing samples\"):\n",
    "    try:\n",
    "        prob = detector.compute_prob(row['text'])\n",
    "        all_probs.append(prob)\n",
    "        true_labels.append(0 if row['models'] == 'human' else 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample (index {index}): {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "# --- Evaluating Results ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"      Fast-DetectGPT Classification Statistics\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(all_probs) > 0 and len(set(true_labels)) > 1:\n",
    "    # Convert probabilities to binary predictions for classification report\n",
    "    binary_predictions = [1 if p > 0.5 else 0 for p in all_probs]\n",
    "\n",
    "    # 1. Classification Report (Precision, Recall, F1-Score)\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(true_labels, binary_predictions, target_names=['Human', 'AI']))\n",
    "\n",
    "    # 2. Overall Accuracy\n",
    "    accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # 3. ROC AUC Score\n",
    "    roc_auc = roc_auc_score(true_labels, all_probs)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # 4. Confusion Matrix and FPR\n",
    "    cm = confusion_matrix(true_labels, binary_predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    print(f\"{'':<15} | {'Predicted Human':<15} | {'Predicted AI':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Actual Human':<15} | {tn:<15} | {fp:<15}\")\n",
    "    print(f\"{'Actual AI':<15} | {fn:<15} | {tp:<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f} (Human text incorrectly flagged as AI)\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    results_df = pd.DataFrame({'true_label': true_labels, 'predicted_prob_ai': all_probs})\n",
    "    results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Detailed results saved to {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(f\"Could not compute metrics. Processed {len(all_probs)} samples.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RishiSurge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
